{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "orig_nbformat": 4,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.2 64-bit"
    },
    "interpreter": {
      "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
    },
    "colab": {
      "name": "DL_project_new_attempt.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMyVS8OCuX23",
        "outputId": "724f8cb1-fdf5-496d-e1bf-01c131333138"
      },
      "source": [
        "!wget https://market1501.s3-us-west-2.amazonaws.com/dataset.zip\n",
        "!unzip -q dataset.zip -d dataset\n",
        "\n",
        "!rm -rf /content/deeplearning_unitn\n",
        "!git clone https://github.com/deayalar/deeplearning_unitn.git\n",
        "\n",
        "!nvidia-smi\n",
        "\n",
        "%cd /content/deeplearning_unitn"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-16 22:02:21--  https://market1501.s3-us-west-2.amazonaws.com/dataset.zip\n",
            "Resolving market1501.s3-us-west-2.amazonaws.com (market1501.s3-us-west-2.amazonaws.com)... 52.218.236.129\n",
            "Connecting to market1501.s3-us-west-2.amazonaws.com (market1501.s3-us-west-2.amazonaws.com)|52.218.236.129|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 82925180 (79M) [application/zip]\n",
            "Saving to: ‘dataset.zip.2’\n",
            "\n",
            "dataset.zip.2       100%[===================>]  79.08M  29.3MB/s    in 2.7s    \n",
            "\n",
            "2021-06-16 22:02:24 (29.3 MB/s) - ‘dataset.zip.2’ saved [82925180/82925180]\n",
            "\n",
            "replace dataset/annotations_train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n",
            "Cloning into 'deeplearning_unitn'...\n",
            "remote: Enumerating objects: 135, done.\u001b[K\n",
            "remote: Counting objects: 100% (135/135), done.\u001b[K\n",
            "remote: Compressing objects: 100% (85/85), done.\u001b[K\n",
            "remote: Total 135 (delta 71), reused 102 (delta 43), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (135/135), 5.37 MiB | 13.04 MiB/s, done.\n",
            "Resolving deltas: 100% (71/71), done.\n",
            "Wed Jun 16 22:02:48 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   58C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n",
            "/content/deeplearning_unitn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4B4h13ssQja"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from cost_functions import cross_entropy\n",
        "import cost_functions\n",
        "from evaluation import Evaluator\n",
        "from my_datasets.reid_dataset import Market1501\n",
        "from utils.split_data import ValidationSplitter, TrainingSplitter\n",
        "from models.reid_model import ReIdModel, FinetunedModel\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFwRMSn4sQjl"
      },
      "source": [
        "config = dict(\n",
        "    wandb = False,\n",
        "    device = \"auto\", # Select an specific device None to select automatically\n",
        "    train_root = \"/content/dataset/train\",\n",
        "    test_root = \"/content/dataset/test\",\n",
        "    queries_root = \"/content/dataset/queries\",\n",
        "    attributes_file = \"/content/dataset/annotations_train.csv\",\n",
        "    # train_root = \"/media/deayalar/Data/Documents/Unitn/Deep Learning/Assignment/dataset/train\",\n",
        "    # test_root = \"/media/deayalar/Data/Documents/Unitn/Deep Learning/Assignment/dataset/test\",\n",
        "    # queries_root = \"/media/deayalar/Data/Documents/Unitn/Deep Learning/Assignment/dataset/queries\",\n",
        "    # attributes_file = \"/media/deayalar/Data/Documents/Unitn/Deep Learning/Assignment/dataset/annotations_train.csv\",\n",
        "    dataset=\"Market1501\",\n",
        "    backbone = \"resnet18\",\n",
        "    loss = cross_entropy(),\n",
        "    split = dict(\n",
        "        full_training_size = 0.75,\n",
        "        train_size = 0.8\n",
        "    ),\n",
        "    compose = dict(\n",
        "        resize_h = 224,\n",
        "        resize_w = 224\n",
        "    ),\n",
        "    epochs=1,\n",
        "    training_batch_size=32,\n",
        "    validation_batch_size=32,\n",
        "    learning_rate=0.01,\n",
        "    weight_decay=0.000001, \n",
        "    momentum=0.9, \n",
        "    mAP_rank=10)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ooXHR_rsQjq",
        "outputId": "cd704cc9-f063-42dc-9f03-17473ec60d2a"
      },
      "source": [
        "def split_data(config):\n",
        "    print('split_data OK!')\n",
        "    splitter = ValidationSplitter(train_root=config[\"train_root\"], \n",
        "                                  test_root=config[\"test_root\"], \n",
        "                                  queries_root=config[\"queries_root\"])\n",
        "    train_set, val_set, val_queries = splitter.split(train_size=config[\"split\"][\"full_training_size\"],\n",
        "                                                     random_seed=42)\n",
        "    return train_set, val_set, val_queries\n",
        "train_set, val_set, val_queries = split_data(config)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "split_data OK!\n",
            "Extract queries proportion: 0.11\n",
            "Identities in train set: 563\n",
            "Identities in validation set: 188\n",
            "Train set size: 9721\n",
            "Validation set size: 2908\n",
            "Number of validation queries: 360\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TdqcTN7sQjx"
      },
      "source": [
        "from my_datasets.reid_dataset import Market1501\n",
        "\n",
        "#Create pytorch Datasets \n",
        "composed = transforms.Compose([transforms.Resize((config[\"compose\"][\"resize_h\"], \n",
        "                                                  config[\"compose\"][\"resize_w\"])),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                    std=[0.229, 0.224, 0.225])])\n",
        "\n",
        "train_dataset = Market1501(root_dir=config[\"train_root\"],\n",
        "                        attributes_file=config[\"attributes_file\"],\n",
        "                        #full_train_set=full_train_set,\n",
        "                        images_list=train_set,\n",
        "                        transform=composed)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f07Tp3tYsQj5",
        "outputId": "d678fff5-53da-45e9-e61e-f05625cb8962"
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "                                            batch_size=config[\"training_batch_size\"], \n",
        "                                            shuffle=True, \n",
        "                                            # num_workers=8\n",
        "                                            num_workers=4)\n",
        "                                            \n",
        "# val_loader = torch.utils.data.DataLoader(val_dataset, \n",
        "#                                             batch_size=config[\"validation_batch_size\"], \n",
        "#                                             shuffle=False, \n",
        "#                                             num_workers=8)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06g_Zdqc47lh"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchvision import models\n",
        "\n",
        "PRETRAINED_MODELS = {\n",
        "    \"resnet18\": { \"load\": lambda : models.resnet18(pretrained=True), \"feature_size\": 512},\n",
        "    \"resnet50\": { \"load\": lambda : models.resnet50(pretrained=True), \"feature_size\": 2048}\n",
        "    # More pretrained models here e.g. alexnet, vgg16, etc\n",
        "}\n",
        "\n",
        "class FinetunedModel(nn.Module):\n",
        "    def __init__(self, architecture, n_classes):\n",
        "        super(FinetunedModel, self).__init__()\n",
        "        self.architecture = architecture\n",
        "\n",
        "        self.backbone = PRETRAINED_MODELS[architecture][\"load\"]()\n",
        "        self.feature_size = PRETRAINED_MODELS[architecture][\"feature_size\"]\n",
        "        print(f\"Backbone feature size: {self.feature_size}\")\n",
        "        self.finetune(self.backbone, n_classes)\n",
        "\n",
        "    def finetune(self, model, n_classes):\n",
        "        model_name = model.__class__.__name__\n",
        "        if model_name.lower().startswith(\"resnet\"):\n",
        "            self.features = nn.Sequential(*list(model.children())[:-1])\n",
        "            self.classifier = nn.Sequential(\n",
        "                nn.Linear(self.feature_size, n_classes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x, get_features=False):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        if get_features:\n",
        "            return x\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class ReIdModel(nn.Module):\n",
        "    \"Model based on LeNet for person re-identification\"\n",
        "    def __init__(self, n_classes):\n",
        "        super().__init__()\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5, stride=1),\n",
        "            nn.BatchNorm2d(6),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2),\n",
        "            nn.Conv2d(in_channels=16, out_channels=120, kernel_size=5, stride=1),\n",
        "            nn.BatchNorm2d(120),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(in_features=7 * 7 * 120, out_features=120),\n",
        "            nn.BatchNorm1d(120),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=120, out_features=84),\n",
        "            nn.BatchNorm1d(84),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(in_features=84, out_features=n_classes),\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.feature_extractor(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OATI96yHsQj-"
      },
      "source": [
        "'''\n",
        "Let's take the backbone. weights trained. \n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "def initialize_resnet18(num_classes):\n",
        "    model_conv = torchvision.models.resnet18(pretrained=True)\n",
        "\n",
        "    num_ftrs = model_conv.fc.in_features\n",
        "    model_conv.fc = nn.Linear(num_ftrs, num_classes)\n",
        "    return model_conv\n",
        "\n",
        "\n",
        "# def initialize_alexnet(num_classes):\n",
        "#     # load the pre-trained Alexnet\n",
        "#     alexnet = torchvision.models.alexnet(pretrained=True)\n",
        "  \n",
        "#     # get the number of neurons in the penultimate layer\n",
        "#     in_features = alexnet.classifier[6].in_features\n",
        "    \n",
        "#     # re-initalize the output layer\n",
        "#     alexnet.classifier[6] = torch.nn.Linear(in_features=in_features, \n",
        "#                                             out_features=num_classes)\n",
        "    \n",
        "#     return alexnet\n",
        "\n",
        "\n",
        "def get_cost_function():\n",
        "    cost_function = torch.nn.CrossEntropyLoss()\n",
        "    return cost_function\n",
        "\n",
        "def get_optimizer(model, lr, wd, momentum):\n",
        "  \n",
        "    # we will create two groups of weights, one for the newly initialized layer\n",
        "    # and the other for rest of the layers of the network\n",
        "    \n",
        "    final_layer_weights = []\n",
        "    rest_of_the_net_weights = []\n",
        "    \n",
        "    # we will iterate through the layers of the network\n",
        "    for name, param in model.named_parameters():\n",
        "        if name.startswith('classifier.6'):\n",
        "            final_layer_weights.append(param)\n",
        "        else:\n",
        "            rest_of_the_net_weights.append(param)\n",
        "    \n",
        "    # so now we have divided the network weights into two groups.\n",
        "    # We will train the final_layer_weights with learning_rate = lr\n",
        "    # and rest_of_the_net_weights with learning_rate = lr / 10\n",
        "    \n",
        "    optimizer = torch.optim.SGD([\n",
        "        {'params': rest_of_the_net_weights},\n",
        "        {'params': final_layer_weights, 'lr': lr}\n",
        "    ], lr=lr / 10, weight_decay=wd, momentum=momentum)\n",
        "    \n",
        "    return optimizer\n",
        "\n",
        "\n",
        "def train(net,data_loader,optimizer,cost_function, device='cuda:0'):\n",
        "    samples = 0.\n",
        "    cumulative_loss = 0.\n",
        "    cumulative_accuracy = 0.\n",
        "\n",
        "    \n",
        "    net.train() # Strictly needed if network contains layers which has different behaviours between train and test\n",
        "    # for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "    for  (batch_idx,targets) in enumerate(data_loader):\n",
        "        # print('inputs: ',targets[0].size(),'labels: ', targets[1].size())\n",
        "\n",
        "        t1 =targets[1].to(device)\n",
        "\n",
        "        # Load data into GPU\n",
        "        inputs = targets[0].to(device)\n",
        "        # inputs = targets[0]\n",
        "        targets = targets[2].to(device)\n",
        "        # targets = targets[2]\n",
        "        \n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = net(inputs)\n",
        "        # print('outputs: ',outputs.size())\n",
        "\n",
        "        # Apply the loss\n",
        "        # loss = cost_function(outputs,targets)\n",
        "        loss = cost_function(outputs, torch.max(targets, 1)[1])\n",
        "        # loss = criterion(outputs, torch.max(labels, 1)[1])\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        \n",
        "        # Update parameters\n",
        "        optimizer.step()\n",
        "        \n",
        "        # Resets the gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Better print something, no?\n",
        "        samples+=inputs.shape[0]\n",
        "        cumulative_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        # try:\n",
        "        #   print(predicted.size())\n",
        "        #   print(t1.size())\n",
        "        #   print(predicted.eq(t1))\n",
        "        # except:\n",
        "        #   print(targets[0].size())\n",
        "        #   print(predicted.eq(targets[0]))\n",
        "\n",
        "        cumulative_accuracy += predicted.eq(t1).sum().item()\n",
        "        # cumulative_accuracy += predicted.eq(targets[1]).sum()\n",
        "\n",
        "    return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-GHePOgsQkP"
      },
      "source": [
        "def main(train_loader,\n",
        "        batch_size=128, \n",
        "         device='cuda:0', \n",
        "         learning_rate=0.001, \n",
        "         weight_decay=0.000001, \n",
        "         momentum=0.9, \n",
        "         epochs=50, \n",
        "         num_classes=27\n",
        "         ):\n",
        "  \n",
        "  #   writer = SummaryWriter(log_dir=\"runs/exp1\")\n",
        "\n",
        "  # Instantiates dataloaders\n",
        "  #   train_loader, test_loader = get_data(batch_size=batch_size, img_root=img_root)\n",
        "  \n",
        "  # Instantiates the model\n",
        "  net = initialize_resnet18(num_classes=num_classes).to(device)\n",
        "  \n",
        "  # Instantiates the optimizer\n",
        "  optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
        "  \n",
        "  # Instantiates the cost function\n",
        "  cost_function = get_cost_function()\n",
        "\n",
        "  #   print('Before training:')\n",
        "#   train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "#   test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "#   print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "#   print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "#   print('-----------------------------------------------------')\n",
        "  \n",
        "  # Add values to plots\n",
        "#   writer.add_scalar('Loss/train_loss', train_loss, 0)\n",
        "#   writer.add_scalar('Loss/test_loss', test_loss, 0)\n",
        "#   writer.add_scalar('Accuracy/train_accuracy', train_accuracy, 0)\n",
        "#   writer.add_scalar('Accuracy/test_accuracy', test_accuracy, 0)\n",
        "\n",
        "  for e in range(epochs):\n",
        "    train_loss, train_accuracy = train(net, train_loader, optimizer, cost_function)\n",
        "    # test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    # print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "    \n",
        "    # Add values to plots\n",
        "    # writer.add_scalar('Loss/train_loss', train_loss, e + 1)\n",
        "    # writer.add_scalar('Loss/test_loss', test_loss, e + 1)\n",
        "    # writer.add_scalar('Accuracy/train_accuracy', train_accuracy, e + 1)\n",
        "    # writer.add_scalar('Accuracy/test_accuracy', test_accuracy, e + 1)\n",
        "\n",
        "  print('After training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "#   test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "#   print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "  # Closes the logger\n",
        "#   writer.close()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "id": "2UTN4GVdsQkS",
        "outputId": "104f4779-5e94-4ed8-da61-d4974a157ae2"
      },
      "source": [
        "main(train_loader,\n",
        "        batch_size=32,\n",
        "        device='cuda:0', \n",
        "         learning_rate=0.001, \n",
        "         weight_decay=0.000001, \n",
        "         momentum=0.9, \n",
        "         epochs=50, \n",
        "         num_classes=27\n",
        "         )"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "\t Training loss 0.00648, Training accuracy 0.16\n",
            "-----------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-27c1376c113e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m          \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m          \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m          \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m27\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m          )\n",
            "\u001b[0;32m<ipython-input-36-23070cc70875>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(train_loader, batch_size, device, learning_rate, weight_decay, momentum, epochs, num_classes)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;31m# test_loss, test_accuracy = test(net, test_loader, cost_function)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch: {:d}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-35-9f7730fe9a86>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, data_loader, optimizer, cost_function, device)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# Better print something, no?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0msamples\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mcumulative_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;31m# try:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYV1eiYh2L_D"
      },
      "source": [
        "# NOT HERE YET!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g0JwAvjFsQkU"
      },
      "source": [
        "from utils import image_utils\n",
        "image_utils.imshow(train_dataset[10][0])\n",
        "print(train_dataset[10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J22ieMz9sQkW"
      },
      "source": [
        "print(len(train_dataset.attr))\n",
        "print(len(train_dataset.class_to_idx))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "vkYRBYYFsQkX"
      },
      "source": [
        "from utils import image_utils\n",
        "print(train_dataset.images_list[0])\n",
        "print(image_utils.get_ids_from_images(train_dataset.images_list)[0])\n",
        "image_utils.imshow(train_dataset[0][0])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NV0DzUewsQkY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzJDMFIEsQkc"
      },
      "source": [
        "train_dataset.__init__(root_dir=config[\"train_root\"],\n",
        "                        attributes_file=config[\"attributes_file\"], images_list=img_l).__getitem__()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCmhcwfFsQki"
      },
      "source": [
        "img_l = train_dataset.images_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2238TyvsQkr"
      },
      "source": [
        "train_dataset.__getitem__(int('0901'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KfDgZevsQks"
      },
      "source": [
        "train_dataset.__getitem__(int('0901'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jbBNuDKdsQku"
      },
      "source": [
        "idx = train_dataset.class_to_idx.keys()\n",
        "# idx\n",
        "train_dataset.__getitem__(int('0901'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ2M7SIdsQky"
      },
      "source": [
        "train_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H5NSSSYXsQk0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}