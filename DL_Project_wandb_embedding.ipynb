{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL Project wandb embedding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.7.10 64-bit ('dl_unitn': conda)",
      "name": "python371064bitdlunitncondad5dd323ef49745509346e8124c4613ec"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10-final"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deayalar/deeplearning_unitn/blob/main/DL_Project_wandb_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YOHCjjwPGWc",
        "outputId": "24fc2c0a-9bb1-4da5-c5bf-2acf03620a21"
      },
      "source": [
        "!wget https://market1501.s3-us-west-2.amazonaws.com/dataset.zip\n",
        "!unzip -q dataset.zip -d dataset"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-22 16:23:44--  https://market1501.s3-us-west-2.amazonaws.com/dataset.zip\n",
            "Resolving market1501.s3-us-west-2.amazonaws.com (market1501.s3-us-west-2.amazonaws.com)... 52.218.233.17\n",
            "Connecting to market1501.s3-us-west-2.amazonaws.com (market1501.s3-us-west-2.amazonaws.com)|52.218.233.17|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 82925180 (79M) [application/zip]\n",
            "Saving to: ‘dataset.zip’\n",
            "\n",
            "dataset.zip         100%[===================>]  79.08M  36.2MB/s    in 2.2s    \n",
            "\n",
            "2021-06-22 16:23:47 (36.2 MB/s) - ‘dataset.zip’ saved [82925180/82925180]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_--0IpcYHBC_",
        "outputId": "f8c572fb-353f-4dc2-e869-20f365d7bb0d"
      },
      "source": [
        "!rm -rf /content/deeplearning_unitn\n",
        "!git clone https://github.com/deayalar/deeplearning_unitn.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'deeplearning_unitn'...\n",
            "remote: Enumerating objects: 177, done.\u001b[K\n",
            "remote: Counting objects: 100% (177/177), done.\u001b[K\n",
            "remote: Compressing objects: 100% (116/116), done.\u001b[K\n",
            "remote: Total 177 (delta 98), reused 117 (delta 51), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (177/177), 5.49 MiB | 9.58 MiB/s, done.\n",
            "Resolving deltas: 100% (98/98), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUb-UELw5R_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db606f6d-bef4-4e31-f6e5-1b40e4fb41d1"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Jun 22 16:23:53 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   56C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-GfOs_XIJGN"
      },
      "source": [
        "config = dict(\n",
        "    wandb = True,\n",
        "    device = \"auto\", # Select an specific device None to select automatically\n",
        "    train_root = \"/content/dataset/train\",\n",
        "    test_root = \"/content/dataset/test\", \n",
        "    queries_root = \"/content/dataset/queries\",\n",
        "    attributes_file = \"/content/dataset/annotations_train.csv\",\n",
        "    #train_root = \"/media/deayalar/Data/Documents/Unitn/Deep Learning/Assignment/dataset/train\",\n",
        "    #test_root = \"/media/deayalar/Data/Documents/Unitn/Deep Learning/Assignment/dataset/test\",\n",
        "    #queries_root = \"/media/deayalar/Data/Documents/Unitn/Deep Learning/Assignment/dataset/queries\",\n",
        "    #attributes_file = \"/media/deayalar/Data/Documents/Unitn/Deep Learning/Assignment/dataset/annotations_train.csv\",\n",
        "    dataset=\"Market1501\",\n",
        "    backbone = \"resnet18\",\n",
        "    split = dict(\n",
        "        full_training_size = 0.75,\n",
        "        train_size = 0.8\n",
        "    ),\n",
        "    compose = dict(\n",
        "        resize_h = 224,\n",
        "        resize_w = 224\n",
        "    ),\n",
        "    epochs=1,\n",
        "    training_batch_size=8,\n",
        "    validation_batch_size=32,\n",
        "    learning_rate=0.01,\n",
        "    weight_decay=0.000001, \n",
        "    momentum=0.9,\n",
        "    test_before_training=True,\n",
        "    test_after_epochs=10,\n",
        "    mAP_rank=10)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpj16wGSIgxr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bce3130c-ee4f-4f53-91d1-b1b083b94030"
      },
      "source": [
        "%cd /content/deeplearning_unitn\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import cost_functions\n",
        "from evaluation import Evaluator\n",
        "from my_datasets.reid_dataset import Market1501\n",
        "from cost_functions import AttributesLossWrapper\n",
        "from utils.split_data import ValidationSplitter, TrainingSplitter\n",
        "from models.reid_model import FinetunedModel\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "if config[\"device\"] == \"auto\":\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "else:\n",
        "    device = config[\"device\"]\n",
        "print(device)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/deeplearning_unitn\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE5e6DB4Igxs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "42c5aec2-2873-406e-e5e8-e03313899ea0"
      },
      "source": [
        "!pip install wandb -q\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dm98oBsFEHio"
      },
      "source": [
        "def model_pipeline(hyperparameters):\n",
        "    \"\"\"\n",
        "    This function corresponds to the basic pipeline of all tested models\n",
        "    0) Split data\n",
        "    1) Setup based on the configuration\n",
        "    2) Train the model\n",
        "    3) Test performance\n",
        "    \"\"\"\n",
        "    config = hyperparameters\n",
        "    if config[\"wandb\"]:\n",
        "      wandb.init(entity=\"dl_unitn\", project=\"dl_project\", config=hyperparameters)\n",
        "      config = wandb.config\n",
        "    print(config)\n",
        "    \n",
        "    train_set, val_set, val_queries = split_data(config)\n",
        "    \n",
        "    model, train_loader, val_loader, val_queries_loader, criterion, optimizer = setup(train_set, val_set, val_queries, config)\n",
        "    id_ground_truth_dict = build_ground_truth(val_set, val_queries)\n",
        "\n",
        "    print(\"Using \"+ config[\"backbone\"] + \" as backbone\")\n",
        "    if config[\"test_before_training\"]:\n",
        "      test(model, val_loader, val_queries_loader, id_ground_truth_dict, config)\n",
        "\n",
        "    train(model, train_loader, val_loader, criterion, optimizer, config)\n",
        "\n",
        "    test(model, val_loader, val_queries_loader, id_ground_truth_dict, config, save_model=True)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBoOUBtPIgxu"
      },
      "source": [
        "def build_ground_truth(val_set, val_queries):\n",
        "    values = []\n",
        "    for q in val_queries:\n",
        "        matches = []\n",
        "        for idx_v, v in enumerate(val_set):\n",
        "            if v.split(\"_\")[0] == q.split(\"_\")[0]:\n",
        "                matches.append(idx_v)\n",
        "        value = set(matches)\n",
        "        values.append(value)\n",
        "        \n",
        "    ground_truth_dict = dict(zip(list(range(0, len(val_queries))), values))\n",
        "    return ground_truth_dict\n"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0fYIjoLG2rv"
      },
      "source": [
        "def split_data(config):\n",
        "    \"\"\"Returns a list with the names of theimages in each set\"\"\"\n",
        "    splitter = ValidationSplitter(train_root=config[\"train_root\"], \n",
        "                                  test_root=config[\"test_root\"], \n",
        "                                  queries_root=config[\"queries_root\"])\n",
        "    train_set, val_set, val_queries = splitter.split(train_size=config[\"split\"][\"full_training_size\"],\n",
        "                                                     random_seed=42)\n",
        "    return train_set, val_set, val_queries\n",
        "\n",
        "def setup(train_set, val_set, val_queries, config):\n",
        "    #Create pytorch Datasets \n",
        "    composed = transforms.Compose([transforms.Resize((config[\"compose\"][\"resize_h\"], \n",
        "                                                      config[\"compose\"][\"resize_w\"])),\n",
        "                                   transforms.ToTensor(),\n",
        "                                   transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                        std=[0.229, 0.224, 0.225])])\n",
        "    \n",
        "    train_dataset = Market1501(root_dir=config[\"train_root\"],\n",
        "                            attributes_file=config[\"attributes_file\"],\n",
        "                            #full_train_set=full_train_set,\n",
        "                            images_list=train_set,\n",
        "                            transform=composed)\n",
        "                            \n",
        "    val_dataset = Market1501(root_dir=config[\"train_root\"],\n",
        "                         attributes_file=config[\"attributes_file\"],\n",
        "                         #full_train_set = full_train_set,\n",
        "                         images_list=val_set,\n",
        "                         transform=composed)\n",
        "\n",
        "    val_queries_dataset = Market1501(root_dir=config[\"train_root\"],\n",
        "                         attributes_file=config[\"attributes_file\"],\n",
        "                         #full_train_set = full_train_set,\n",
        "                         images_list=val_queries,\n",
        "                         transform=composed)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "                                               batch_size=config[\"training_batch_size\"], \n",
        "                                               shuffle=True, \n",
        "                                               num_workers=8)\n",
        "                                               \n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, \n",
        "                                             batch_size=config[\"validation_batch_size\"], \n",
        "                                             shuffle=False, \n",
        "                                             num_workers=8)\n",
        "\n",
        "    val_queries_loader = torch.utils.data.DataLoader(val_queries_dataset, \n",
        "                                             batch_size=config[\"validation_batch_size\"],\n",
        "                                             shuffle=False, \n",
        "                                             num_workers=8)\n",
        "\n",
        "    attr_len = len(train_dataset[0][2]) #Number of attributes in the csv: 27\n",
        "    print(f\"Number of attributes: {attr_len}\")\n",
        "    model = FinetunedModel(architecture=config[\"backbone\"] ,n_classes=attr_len).to(device)\n",
        "    \n",
        "    criterion = AttributesLossWrapper(2) #TODO: So far it optimizes only the attributes but this loss should be the combination of identification and attributes\n",
        "    optimizer = torch.optim.SGD(model.parameters(), \n",
        "                                lr=config[\"learning_rate\"], \n",
        "                                weight_decay=config[\"weight_decay\"], \n",
        "                                momentum=config[\"momentum\"])\n",
        "    \n",
        "    return model, train_loader, val_loader, val_queries_loader, criterion, optimizer"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0FUbfkUPZJb"
      },
      "source": [
        "def train(model, train_loader, val_loader, criterion, optimizer, config):\n",
        "    # tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
        "    if config[\"wandb\"]:\n",
        "        wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
        "    model.train()\n",
        "    # Run training and track with wandb\n",
        "    total_batches = len(train_loader) * config[\"epochs\"]\n",
        "    example_ct = 0  # number of seen examples\n",
        "    batch_ct = 0\n",
        "\n",
        "    for epoch in tqdm(range(config[\"epochs\"])):\n",
        "        for batch_idx, (inputs, identity, attributes) in enumerate(train_loader):\n",
        "            loss, predicted = train_batch(inputs, identity, attributes, model, optimizer, criterion)\n",
        "\n",
        "            example_ct +=  len(inputs)\n",
        "            batch_ct += 1\n",
        "\n",
        "            if ((batch_ct + 1) % 50) == 0:\n",
        "                train_log(loss, example_ct, epoch)\n",
        "\n",
        "\n",
        "def train_batch(inputs, identity, attributes, model, optimizer, criterion):\n",
        "    inputs = inputs.to(device)\n",
        "    attributes = attributes.to(device)\n",
        "    \n",
        "    # Forward pass\n",
        "    outputs = model(inputs)\n",
        "    # Apply the loss\n",
        "    loss = criterion(outputs, attributes)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Step with optimizer\n",
        "    optimizer.step()\n",
        "\n",
        "    # Zeros the gradient\n",
        "    predicted = torch.round(outputs[0].squeeze(1))\n",
        "\n",
        "    return loss, predicted"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdfG05psTqWb"
      },
      "source": [
        "def train_log(loss, example_ct, epoch):\n",
        "    loss = float(loss)\n",
        "    if config[\"wandb\"]:\n",
        "        wandb.log({\"epoch\": epoch, \"loss\": loss}, step=example_ct)\n",
        "    print(f\"Epoch {epoch}: Loss after \" + str(example_ct).zfill(5) + f\" examples: {loss:.3f}\")"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkgcTy5MIgx3"
      },
      "source": [
        "def get_features_from_loader(model, loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        all_features = torch.zeros(len(loader.dataset), model.feature_size)\n",
        "        for batch_idx, (inputs, ids, attr) in enumerate(tqdm(loader)):\n",
        "                inputs = inputs.to(device)\n",
        "                features = model(inputs, get_features=True)\n",
        "                for in_batch, f in enumerate(features):\n",
        "                    all_features[(batch_idx * loader.batch_size) + in_batch] = f\n",
        "        return all_features\n",
        "\n",
        "\n",
        "def test_mAP(model, gallery_loader, queries_loader, ground_truth_dict, config, save_model=False):\n",
        "    \"\"\"\n",
        "    This function returns the mAP performance of a given model \n",
        "    Params:\n",
        "    model: model to be evaluated\n",
        "    gallery: tensor that contains the feature representations of the target images in validation or test set\n",
        "    queries: tensor that contains feature representations of the queries\n",
        "    rank: top number of elements to retrieve\n",
        "\n",
        "    Returns:\n",
        "    mAP performance of the model\n",
        "    \"\"\"\n",
        "\n",
        "    # Run the model on some test examples\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        # Compute the features for queries and gallery\n",
        "        print(\"Computing gallery features...\")\n",
        "        gallery_features = get_features_from_loader(model, gallery_loader)\n",
        "        print(\"Computing query features...\")\n",
        "        query_features = get_features_from_loader(model, queries_loader)\n",
        "        \n",
        "        # Build the cosine similarity matrix between the all the queries and all the elements in gallery\n",
        "        print(\"Computing cosine similarities...\")\n",
        "        sims_matrix = torch.empty(query_features.size()[0], gallery_features.size()[0])\n",
        "        for idx, q in enumerate(query_features):\n",
        "            sims_matrix[idx] = F.cosine_similarity(q, gallery_features, dim=-1)\n",
        "        \n",
        "        print(\"Similarity matrix shape: \" + str(sims_matrix.size()))\n",
        "        sorted_index = torch.argsort(sims_matrix, dim=1, descending=True)\n",
        "        top_k = sorted_index.narrow_copy(dim=1, start=0, length=config[\"mAP_rank\"])\n",
        "\n",
        "        #Build the dictionary to compute the mAP\n",
        "        predictions_dict = {idx:  r for idx, r in enumerate(top_k.tolist())}\n",
        "        mAP = Evaluator.evaluate_map(predictions_dict, ground_truth_dict)\n",
        "        \n",
        "        print(f\"mAP: {mAP}\")\n",
        "        if config[\"wandb\"]:\n",
        "            wandb.log({\"mAP\": mAP})"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqwJwWFgIgx4"
      },
      "source": [
        "def get_attributes_from_loader(model, loader):\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = np.empty(shape=[0, 27], dtype=np.byte)\n",
        "    all_attrs = np.empty(shape=[0, 27], dtype=np.byte)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, ids, attr) in enumerate(tqdm(loader)):\n",
        "                inputs = inputs.to(device)\n",
        "                outputs = model(inputs, get_features=False)\n",
        "                #print(\"attr:\",attr)\n",
        "                predictions = torch.empty(attr.size()[1], attr.size()[0])\n",
        "                for attr_idx, output in enumerate(outputs):\n",
        "                    if output.size()[1] == 1: #If the output is binary\n",
        "                        pred = torch.round(torch.squeeze(output, 1))\n",
        "                    else: #Otherwise it is multiclass\n",
        "                        pred = torch.argmax(output, dim=1)\n",
        "                    predictions[attr_idx] = pred\n",
        "\n",
        "                predictions = torch.transpose(predictions, 0, 1).cpu().numpy()\n",
        "                attr = attr.cpu().numpy()\n",
        "\n",
        "                all_predictions = np.append(all_predictions, predictions, axis=0)\n",
        "                #print(\"all_predictions shape: \", all_predictions.shape)\n",
        "                all_attrs = np.append(all_attrs, attr, axis=0)\n",
        "                #print(\"all_attrs shape: \", all_attrs.shape)\n",
        "        return all_predictions, all_attrs\n",
        "\n",
        "def test_attributes(model, loader, config):\n",
        "    print(\"Computing attributes...\")\n",
        "    predictions, attr = get_attributes_from_loader(model, loader)\n",
        "    print(\"pred shape: \", predictions.shape)\n",
        "    print(\"attr shape: \", attr.shape)\n",
        "\n",
        "    accuracy_list = []\n",
        "    precision_list = []\n",
        "    recall_list = []\n",
        "    f1_score_list = []\n",
        "\n",
        "    for i in range(0, predictions.shape[1]):\n",
        "        y_true, y_pred = attr[:, i], predictions[:, i]\n",
        "        accuracy_list.append(accuracy_score(y_true, y_pred))\n",
        "        if i == 0: #If it is age\n",
        "            precision_list.append(precision_score(y_true, y_pred, average='macro'))\n",
        "            recall_list.append(recall_score(y_true, y_pred, average='macro'))\n",
        "            f1_score_list.append(f1_score(y_true, y_pred, average='macro'))\n",
        "        else:\n",
        "            precision_list.append(precision_score(y_true, y_pred))\n",
        "            recall_list.append(recall_score(y_true, y_pred))\n",
        "            f1_score_list.append(f1_score(y_true, y_pred))\n",
        "\n",
        "    average_acc = np.mean(accuracy_list)\n",
        "    average_precision = np.mean(precision_list)\n",
        "    average_recall = np.mean(recall_list)\n",
        "    average_f1score = np.mean(f1_score_list)\n",
        "\n",
        "    print(\"accuracy_list: \", accuracy_list)\n",
        "    print(\"precision_list: \", precision_list)\n",
        "    print(\"recall_list: \", recall_list)\n",
        "    print(\"f1_score_list: \", f1_score_list)\n",
        "\n",
        "    print(\"average_acc: \", average_acc)\n",
        "    print(\"average_precision: \", average_precision)\n",
        "    print(\"average_recall: \", average_recall)\n",
        "    print(\"average_f1score: \", average_f1score)\n",
        "\n",
        "    if config[\"wandb\"]:\n",
        "            wandb.log({\"accuracy_list\": accuracy_list})\n",
        "            wandb.log({\"precision_list\": precision_list})\n",
        "            wandb.log({\"recall_list\": recall_list})\n",
        "            wandb.log({\"f1_score_list\": f1_score_list})\n",
        "\n",
        "            wandb.log({\"average accuracy\": average_acc})\n",
        "            wandb.log({\"average precision\": average_precision})\n",
        "            wandb.log({\"average recall\": average_recall})\n",
        "            wandb.log({\"average f1\": average_f1score})"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN6pjSVOKTCt"
      },
      "source": [
        "def test(model, gallery_loader, queries_loader, ground_truth_dict, config, save_model=False):\n",
        "    print(\"Testing\")\n",
        "    model.eval()\n",
        "\n",
        "    test_mAP(model, gallery_loader, queries_loader, ground_truth_dict, config)\n",
        "    test_attributes(model, gallery_loader, config)\n",
        "\n",
        "    if save_model :\n",
        "      # Save the model in the exchangeable ONNX format\n",
        "      inputs, id, attr = next(iter(gallery_loader))\n",
        "      torch.onnx.export(model, inputs.to(device), \"model.onnx\", True)\n",
        "      if config[\"wandb\"]:\n",
        "        wandb.save(\"model.onnx\")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqtuT4NEPu_a"
      },
      "source": [
        "model = model_pipeline(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0yiRhJNPy5R"
      },
      "source": [
        "#train_set, val_set, val_queries = split_data(config)\n",
        "#model, train_loader, val_loader, val_queries_loader, criterion, optimizer = setup(train_set, val_set, val_queries, config)\n",
        "#id_ground_truth_dict = build_ground_truth(val_set, val_queries)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}