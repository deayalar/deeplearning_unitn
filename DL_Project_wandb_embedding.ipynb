{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of DL Project wandb embedding.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.7.10 64-bit ('dl_unitn': conda)",
      "name": "python371064bitdlunitncondad5dd323ef49745509346e8124c4613ec"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10-final"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "60f514a2a7e64222825cacb363d4027b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a16dbe65e8354b658c5eb574ea6ef31a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b089fb3a178a4e3884c96bc8b9f629f6",
              "IPY_MODEL_a4fb5b9cf90d4ed3b7854ca84048bb21"
            ]
          }
        },
        "a16dbe65e8354b658c5eb574ea6ef31a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b089fb3a178a4e3884c96bc8b9f629f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ef464d546ddc4d029a1239afe89754ed",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 602,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 602,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ed5554759d5f43a3b791712337ae215d"
          }
        },
        "a4fb5b9cf90d4ed3b7854ca84048bb21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d86962b652d3461ca9eb0f5fa3cc6839",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 602/602 [01:32&lt;00:00,  6.51it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9b73eed462c74c7eb80916017f1dc876"
          }
        },
        "ef464d546ddc4d029a1239afe89754ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ed5554759d5f43a3b791712337ae215d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d86962b652d3461ca9eb0f5fa3cc6839": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9b73eed462c74c7eb80916017f1dc876": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5fcf5a24e1b34e26a711f8354b01deaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_aa0c8aa623b341bd9c4e38087682202c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_3508552a29e84ca98b906f41e4a9dc21",
              "IPY_MODEL_0474d8e2057b4fea95721a97bf9667ee"
            ]
          }
        },
        "aa0c8aa623b341bd9c4e38087682202c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3508552a29e84ca98b906f41e4a9dc21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_adca8a999bf84ab18f68e276aa8e20d6",
            "_dom_classes": [],
            "description": "  0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 10,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ee44ceae0bc6473699351a565795a2fd"
          }
        },
        "0474d8e2057b4fea95721a97bf9667ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a53ebf9f2f3d429494eb0880436037ea",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/10 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_67ae3765b4cc4b9197d2d1c39c32b6af"
          }
        },
        "adca8a999bf84ab18f68e276aa8e20d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ee44ceae0bc6473699351a565795a2fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a53ebf9f2f3d429494eb0880436037ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "67ae3765b4cc4b9197d2d1c39c32b6af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deayalar/deeplearning_unitn/blob/main/DL_Project_wandb_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YOHCjjwPGWc",
        "outputId": "5f669f09-8caf-4820-c30e-fa49b89a2b79"
      },
      "source": [
        "!wget https://market1501.s3-us-west-2.amazonaws.com/dataset.zip\n",
        "!unzip -q dataset.zip -d dataset"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-25 13:52:19--  https://market1501.s3-us-west-2.amazonaws.com/dataset.zip\n",
            "Resolving market1501.s3-us-west-2.amazonaws.com (market1501.s3-us-west-2.amazonaws.com)... 52.218.209.57\n",
            "Connecting to market1501.s3-us-west-2.amazonaws.com (market1501.s3-us-west-2.amazonaws.com)|52.218.209.57|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 82925180 (79M) [application/zip]\n",
            "Saving to: ‘dataset.zip’\n",
            "\n",
            "dataset.zip         100%[===================>]  79.08M  52.2MB/s    in 1.5s    \n",
            "\n",
            "2021-06-25 13:52:21 (52.2 MB/s) - ‘dataset.zip’ saved [82925180/82925180]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_--0IpcYHBC_",
        "outputId": "89ebfee5-c9ae-4464-e0ce-62bb6c246c7f"
      },
      "source": [
        "!rm -rf /content/deeplearning_unitn\n",
        "!git clone https://github.com/deayalar/deeplearning_unitn.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'deeplearning_unitn'...\n",
            "remote: Enumerating objects: 245, done.\u001b[K\n",
            "remote: Counting objects: 100% (245/245), done.\u001b[K\n",
            "remote: Compressing objects: 100% (162/162), done.\u001b[K\n",
            "remote: Total 245 (delta 135), reused 138 (delta 62), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (245/245), 10.73 MiB | 5.98 MiB/s, done.\n",
            "Resolving deltas: 100% (135/135), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUb-UELw5R_B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88983ccd-2531-4e81-d068-84305d9035ec"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Jun 25 13:52:59 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-GfOs_XIJGN"
      },
      "source": [
        "config = dict(\n",
        "    wandb = True,\n",
        "    device = \"auto\", # Select an specific device None to select automatically\n",
        "    train_root = \"/content/dataset/train\",\n",
        "    test_root = \"/content/dataset/test\", \n",
        "    queries_root = \"/content/dataset/queries\",\n",
        "    attributes_file = \"/content/dataset/annotations_train.csv\",\n",
        "    #train_root = \"/media/deayalar/Data/Documents/Unitn/Deep Learning/Assignment/dataset/train\",\n",
        "    #test_root = \"/media/deayalar/Data/Documents/Unitn/Deep Learning/Assignment/dataset/test\",\n",
        "    #queries_root = \"/media/deayalar/Data/Documents/Unitn/Deep Learning/Assignment/dataset/queries\",\n",
        "    #attributes_file = \"/media/deayalar/Data/Documents/Unitn/Deep Learning/Assignment/dataset/annotations_train.csv\",\n",
        "    dataset=\"Market1501\",\n",
        "    backbone = \"resnet50\",\n",
        "    split = dict(\n",
        "        full_training_size = 0.75,\n",
        "        train_size = 0.8\n",
        "    ),\n",
        "    compose = dict(\n",
        "        resize_h = 224,\n",
        "        resize_w = 224\n",
        "    ),\n",
        "    epochs=10,\n",
        "    training_batch_size=16,\n",
        "    validation_batch_size=32,\n",
        "    learning_rate=0.01,\n",
        "    weight_decay=0.000001, \n",
        "    momentum=0.9,\n",
        "    test_before_training=False,\n",
        "    test_after_epochs=10,\n",
        "    mAP_rank=15)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpj16wGSIgxr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d79ae0e6-fb55-454b-c37b-69a6d48dbe5d"
      },
      "source": [
        "%cd /content/deeplearning_unitn\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import cost_functions\n",
        "from evaluation import Evaluator\n",
        "from datasets.reid_dataset import Market1501\n",
        "#from cost_functions import OverallLossWrapper\n",
        "from utils.split_data import ValidationSplitter, TrainingSplitter\n",
        "from models.reid_model import FinetunedModel\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "if config[\"device\"] == \"auto\":\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "else:\n",
        "    device = config[\"device\"]\n",
        "print(device)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/deeplearning_unitn\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KE5e6DB4Igxs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08605f28-76d4-4a59-8d5a-da464ef459ed"
      },
      "source": [
        "!pip install wandb -q\n",
        "import wandb\n",
        "if config[\"wandb\"]:\n",
        "  wandb.login()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdeayalar\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dm98oBsFEHio"
      },
      "source": [
        "def model_pipeline(hyperparameters):\n",
        "    \"\"\"\n",
        "    This function corresponds to the basic pipeline of all tested models\n",
        "    0) Split data\n",
        "    1) Setup based on the configuration\n",
        "    2) Train the model\n",
        "    3) Test performance\n",
        "    \"\"\"\n",
        "    config = hyperparameters\n",
        "    if config[\"wandb\"]:\n",
        "      wandb.init(entity=\"dl_unitn\", project=\"dl_project\", config=hyperparameters)\n",
        "      config = wandb.config\n",
        "    print(config)\n",
        "    \n",
        "    train_set, val_set, val_queries = split_data(config)\n",
        "    \n",
        "    model, train_loader, val_loader, val_queries_loader, criterion, optimizer = setup(train_set, val_set, val_queries, config)\n",
        "    id_ground_truth_dict = build_ground_truth(val_set, val_queries)\n",
        "\n",
        "    print(\"Using \"+ config[\"backbone\"] + \" as backbone\")\n",
        "    if config[\"test_before_training\"]:\n",
        "      test(model, val_loader, val_queries_loader, id_ground_truth_dict, config)\n",
        "\n",
        "    train(model, train_loader, val_loader, criterion, optimizer, config)\n",
        "\n",
        "    test(model, val_loader, val_queries_loader, id_ground_truth_dict, config, save_model=True)\n",
        "\n",
        "    return model"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zBoOUBtPIgxu"
      },
      "source": [
        "def build_ground_truth(val_set, val_queries):\n",
        "    values = []\n",
        "    for q in val_queries:\n",
        "        matches = []\n",
        "        for idx_v, v in enumerate(val_set):\n",
        "            if v.split(\"_\")[0] == q.split(\"_\")[0]:\n",
        "                matches.append(idx_v)\n",
        "        value = set(matches)\n",
        "        values.append(value)\n",
        "        \n",
        "    ground_truth_dict = dict(zip(list(range(0, len(val_queries))), values))\n",
        "    return ground_truth_dict\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0fYIjoLG2rv"
      },
      "source": [
        "def split_data(config):\n",
        "    \"\"\"Returns a list with the names of theimages in each set\"\"\"\n",
        "    splitter = ValidationSplitter(train_root=config[\"train_root\"], \n",
        "                                  test_root=config[\"test_root\"], \n",
        "                                  queries_root=config[\"queries_root\"])\n",
        "    train_set, val_set, val_queries = splitter.split(train_size=config[\"split\"][\"full_training_size\"],\n",
        "                                                     random_seed=42)\n",
        "    return train_set, val_set, val_queries\n",
        "\n",
        "def setup(train_set, val_set, val_queries, config):\n",
        "    #Create pytorch Datasets\n",
        "    train_composed = transforms.Compose([ transforms.Resize((config[\"compose\"][\"resize_h\"], \n",
        "                                                      config[\"compose\"][\"resize_w\"])),\n",
        "                                          transforms.RandomHorizontalFlip(),\n",
        "                                          transforms.ToTensor(),\n",
        "                                          transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                                std=[0.229, 0.224, 0.225]),\n",
        "                                          transforms.RandomErasing(p=0.6)])\n",
        "    \n",
        "    val_composed = transforms.Compose([transforms.Resize((config[\"compose\"][\"resize_h\"], \n",
        "                                                      config[\"compose\"][\"resize_w\"])),\n",
        "                                   transforms.ToTensor(),\n",
        "                                   transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                                        std=[0.229, 0.224, 0.225])])\n",
        "    \n",
        "    train_dataset = Market1501(root_dir=config[\"train_root\"],\n",
        "                            attributes_file=config[\"attributes_file\"],\n",
        "                            images_list=train_set,\n",
        "                            transform=train_composed)\n",
        "                            \n",
        "    val_dataset = Market1501(root_dir=config[\"train_root\"],\n",
        "                         attributes_file=config[\"attributes_file\"],\n",
        "                         images_list=val_set,\n",
        "                         transform=val_composed)\n",
        "\n",
        "    val_queries_dataset = Market1501(root_dir=config[\"train_root\"],\n",
        "                         attributes_file=config[\"attributes_file\"],\n",
        "                         images_list=val_queries,\n",
        "                         transform=val_composed)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(train_dataset, \n",
        "                                               batch_size=config[\"training_batch_size\"], \n",
        "                                               shuffle=True, \n",
        "                                               num_workers=2)\n",
        "                                               \n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, \n",
        "                                             batch_size=config[\"validation_batch_size\"], \n",
        "                                             shuffle=False, \n",
        "                                             num_workers=2)\n",
        "\n",
        "    val_queries_loader = torch.utils.data.DataLoader(val_queries_dataset, \n",
        "                                             batch_size=config[\"validation_batch_size\"],\n",
        "                                             shuffle=False, \n",
        "                                             num_workers=2)\n",
        "\n",
        "    attr_len = len(train_dataset[0][2]) #Number of attributes in the csv: 27\n",
        "    print(f\"Number of attributes: {attr_len}\")\n",
        "    model = FinetunedModel(architecture=config[\"backbone\"] ,n_classes=attr_len).to(device)\n",
        "\n",
        "    #This is a combination of the attributes classification loss and the triplet loss for identification\n",
        "    criterion = OverallLossWrapper()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), \n",
        "                                lr=config[\"learning_rate\"], \n",
        "                                weight_decay=config[\"weight_decay\"], \n",
        "                                momentum=config[\"momentum\"])\n",
        "    \n",
        "    return model, train_loader, val_loader, val_queries_loader, criterion, optimizer"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0FUbfkUPZJb"
      },
      "source": [
        "def compute_centroids(model, loader):\n",
        "  model.eval()\n",
        "  print(\"Computing centroids\")\n",
        "  all_features = np.empty((0, model.feature_size))\n",
        "  ids = []\n",
        "  for batch_idx, (inputs, identity, attributes) in enumerate(tqdm(loader)):\n",
        "    inputs = inputs.to(device)\n",
        "    batch_features = model(inputs, get_features = True)\n",
        "    all_features = np.concatenate((all_features, batch_features.cpu().detach().numpy()), axis=0)\n",
        "    ids.extend(list(identity))\n",
        "\n",
        "  ids = np.array([int(i) for i in ids])\n",
        "  unique_ids = np.unique(ids)\n",
        "\n",
        "  centroids = []\n",
        "  for id in unique_ids:\n",
        "    id_features = all_features[ids == id]\n",
        "    centroid = np.mean(id_features, axis = 0)\n",
        "    centroids.append(centroid)\n",
        "  return centroids, unique_ids\n",
        "\n",
        "\n",
        "def train(model, train_loader, val_loader, criterion, optimizer, config):\n",
        "    print(\"Training...\")\n",
        "    print(train_loader)\n",
        "    # tell wandb to watch what the model gets up to: gradients, weights, and more!\n",
        "    if config[\"wandb\"]:\n",
        "         wandb.watch(model, criterion, log=\"all\", log_freq=10)\n",
        "    \n",
        "    # Run training and track with wandb\n",
        "    total_batches = len(train_loader) * config[\"epochs\"]\n",
        "    example_ct = 0  # number of seen examples\n",
        "    batch_ct = 0\n",
        "\n",
        "    centroids, unique_ids = compute_centroids(model, train_loader)\n",
        "\n",
        "    for epoch in tqdm(range(config[\"epochs\"])):\n",
        "        model.train()\n",
        "        for batch_idx, (inputs, identity, attributes) in enumerate(train_loader):\n",
        "            loss = train_batch(inputs, identity, attributes, model, optimizer, criterion, centroids, unique_ids)\n",
        "\n",
        "            example_ct +=  len(inputs)\n",
        "            batch_ct += 1\n",
        "\n",
        "            if ((batch_ct + 1) % 50) == 0:\n",
        "                train_log(loss, example_ct, epoch)\n",
        "\n",
        "        centroids, unique_ids = compute_centroids(model, train_loader)\n",
        "\n",
        "def train_batch(inputs, identity, attributes, model, optimizer, criterion, centroids, unique_ids):\n",
        "    inputs = inputs.to(device)\n",
        "    attributes = attributes.to(device)\n",
        "    \n",
        "    # Forward pass\n",
        "    # TODO: This could be improved in the architecture to return both at the same time and improve the training time\n",
        "    output_attrs = model(inputs)\n",
        "    output_features = model(inputs, get_features=True)\n",
        "\n",
        "    # Filter the centroids and pass only the ones in the batch\n",
        "    centroids_batch = {}\n",
        "    for i in list(identity):\n",
        "      pos = np.flatnonzero(unique_ids==int(i))[0]\n",
        "      centroids_batch[i] = centroids[pos]\n",
        "\n",
        "    # Apply the loss\n",
        "    loss = criterion(output_attrs, attributes, output_features, identity, centroids_batch)\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "\n",
        "    # Step with optimizer\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdfG05psTqWb"
      },
      "source": [
        "def train_log(loss, example_ct, epoch):\n",
        "    loss = float(loss)\n",
        "    if config[\"wandb\"]:\n",
        "        wandb.log({\"epoch\": epoch, \"loss\": loss}, step=example_ct)\n",
        "    print(f\"Epoch {epoch}: Loss after \" + str(example_ct).zfill(5) + f\" examples: {loss:.3f}\")"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkgcTy5MIgx3"
      },
      "source": [
        "def get_features_from_loader(model, loader):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        all_features = torch.zeros(len(loader.dataset), model.feature_size)\n",
        "        for batch_idx, (inputs, ids, attr) in enumerate(tqdm(loader)):\n",
        "                inputs = inputs.to(device)\n",
        "                features = model(inputs, get_features=True)\n",
        "                for in_batch, f in enumerate(features):\n",
        "                    all_features[(batch_idx * loader.batch_size) + in_batch] = f\n",
        "        return all_features\n",
        "\n",
        "\n",
        "def test_mAP(model, gallery_loader, queries_loader, ground_truth_dict, config, save_model=False):\n",
        "    \"\"\"\n",
        "    This function returns the mAP performance of a given model \n",
        "    Params:\n",
        "    model: model to be evaluated\n",
        "    gallery: tensor that contains the feature representations of the target images in validation or test set\n",
        "    queries: tensor that contains feature representations of the queries\n",
        "    rank: top number of elements to retrieve\n",
        "\n",
        "    Returns:\n",
        "    mAP performance of the model\n",
        "    \"\"\"\n",
        "\n",
        "    # Run the model on some test examples\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        # Compute the features for queries and gallery\n",
        "        print(\"Computing gallery features...\")\n",
        "        gallery_features = get_features_from_loader(model, gallery_loader)\n",
        "        print(\"Computing query features...\")\n",
        "        query_features = get_features_from_loader(model, queries_loader)\n",
        "        \n",
        "        # Build the cosine similarity matrix between the all the queries and all the elements in gallery\n",
        "        print(\"Computing cosine similarities...\")\n",
        "        sims_matrix = torch.empty(query_features.size()[0], gallery_features.size()[0])\n",
        "        for idx, q in enumerate(query_features):\n",
        "            sims_matrix[idx] = F.cosine_similarity(q, gallery_features, dim=-1)\n",
        "        \n",
        "        print(\"Similarity matrix shape: \" + str(sims_matrix.size()))\n",
        "        sorted_index = torch.argsort(sims_matrix, dim=1, descending=True)\n",
        "        top_k = sorted_index.narrow_copy(dim=1, start=0, length=config[\"mAP_rank\"])\n",
        "\n",
        "        #Build the dictionary to compute the mAP\n",
        "        predictions_dict = {idx:  r for idx, r in enumerate(top_k.tolist())}\n",
        "        mAP = Evaluator.evaluate_map(predictions_dict, ground_truth_dict)\n",
        "        \n",
        "        print(f\"mAP: {mAP}\")\n",
        "        if config[\"wandb\"]:\n",
        "            wandb.log({\"mAP\": mAP})"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqwJwWFgIgx4"
      },
      "source": [
        "def get_attributes_from_loader(model, loader):\n",
        "    model.eval()\n",
        "\n",
        "    all_predictions = np.empty(shape=[0, 27], dtype=np.byte)\n",
        "    all_attrs = np.empty(shape=[0, 27], dtype=np.byte)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (inputs, ids, attr) in enumerate(tqdm(loader)):\n",
        "                inputs = inputs.to(device)\n",
        "                outputs = model(inputs, get_features=False)\n",
        "                #print(\"attr:\",attr)\n",
        "                predictions = torch.empty(attr.size()[1], attr.size()[0])\n",
        "                for attr_idx, output in enumerate(outputs):\n",
        "                    if output.size()[1] == 1: #If the output is binary\n",
        "                        pred = torch.round(torch.squeeze(output, 1))\n",
        "                    else: #Otherwise it is multiclass\n",
        "                        pred = torch.argmax(output, dim=1)\n",
        "                    predictions[attr_idx] = pred\n",
        "\n",
        "                predictions = torch.transpose(predictions, 0, 1).cpu().numpy()\n",
        "                attr = attr.cpu().numpy()\n",
        "\n",
        "                all_predictions = np.append(all_predictions, predictions, axis=0)\n",
        "                #print(\"all_predictions shape: \", all_predictions.shape)\n",
        "                all_attrs = np.append(all_attrs, attr, axis=0)\n",
        "                #print(\"all_attrs shape: \", all_attrs.shape)\n",
        "        return all_predictions, all_attrs\n",
        "\n",
        "def test_attributes(model, loader, config):\n",
        "    print(\"Computing attributes...\")\n",
        "    predictions, attr = get_attributes_from_loader(model, loader)\n",
        "    print(\"pred shape: \", predictions.shape)\n",
        "    print(\"attr shape: \", attr.shape)\n",
        "\n",
        "    accuracy_list = []\n",
        "    precision_list = []\n",
        "    recall_list = []\n",
        "    f1_score_list = []\n",
        "\n",
        "    for i in range(0, predictions.shape[1]):\n",
        "        y_true, y_pred = attr[:, i], predictions[:, i]\n",
        "        accuracy_list.append(accuracy_score(y_true, y_pred))\n",
        "        if i == 0: #If it is age\n",
        "            precision_list.append(precision_score(y_true, y_pred, average='macro'))\n",
        "            recall_list.append(recall_score(y_true, y_pred, average='macro'))\n",
        "            f1_score_list.append(f1_score(y_true, y_pred, average='macro'))\n",
        "        else:\n",
        "            precision_list.append(precision_score(y_true, y_pred))\n",
        "            recall_list.append(recall_score(y_true, y_pred))\n",
        "            f1_score_list.append(f1_score(y_true, y_pred))\n",
        "\n",
        "    average_acc = np.mean(accuracy_list)\n",
        "    average_precision = np.mean(precision_list)\n",
        "    average_recall = np.mean(recall_list)\n",
        "    average_f1score = np.mean(f1_score_list)\n",
        "\n",
        "    print(\"accuracy_list: \", accuracy_list)\n",
        "    print(\"precision_list: \", precision_list)\n",
        "    print(\"recall_list: \", recall_list)\n",
        "    print(\"f1_score_list: \", f1_score_list)\n",
        "\n",
        "    print(\"average_acc: \", average_acc)\n",
        "    print(\"average_precision: \", average_precision)\n",
        "    print(\"average_recall: \", average_recall)\n",
        "    print(\"average_f1score: \", average_f1score)\n",
        "\n",
        "    if config[\"wandb\"]:\n",
        "            wandb.log({\"accuracy_list\": accuracy_list})\n",
        "            wandb.log({\"precision_list\": precision_list})\n",
        "            wandb.log({\"recall_list\": recall_list})\n",
        "            wandb.log({\"f1_score_list\": f1_score_list})\n",
        "\n",
        "            wandb.log({\"average accuracy\": average_acc})\n",
        "            wandb.log({\"average precision\": average_precision})\n",
        "            wandb.log({\"average recall\": average_recall})\n",
        "            wandb.log({\"average f1\": average_f1score})"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN6pjSVOKTCt"
      },
      "source": [
        "def test(model, gallery_loader, queries_loader, ground_truth_dict, config, save_model=False):\n",
        "    print(\"Testing\")\n",
        "    model.eval()\n",
        "\n",
        "    test_mAP(model, gallery_loader, queries_loader, ground_truth_dict, config)\n",
        "    test_attributes(model, gallery_loader, config)\n",
        "\n",
        "    if save_model :\n",
        "      # Save the model in the exchangeable ONNX format\n",
        "      inputs, id, attr = next(iter(gallery_loader))\n",
        "      torch.onnx.export(model, inputs.to(device), \"model.onnx\", True)\n",
        "      if config[\"wandb\"]:\n",
        "        wandb.save(\"model.onnx\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVOCzV7Wneba"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from scipy.spatial import distance\n",
        "\n",
        "class OverallLossWrapper(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(OverallLossWrapper, self).__init__()\n",
        "        self.id_loss = TripletLoss()\n",
        "        self.attr_loss = AttributesLossWrapper(0)\n",
        "\n",
        "    def forward(self, output_attrs, target_attrs, output_features, target_ids, centroids):\n",
        "        return self.id_loss(output_features, target_ids, centroids) + self.attr_loss(output_attrs, target_attrs)\n",
        "\n",
        "class AttributesLossWrapper(nn.Module):\n",
        "    def __init__(self, task_num):\n",
        "        super(AttributesLossWrapper, self).__init__()\n",
        "        self.task_num = task_num\n",
        "        # This is to learn the weights\n",
        "        #self.log_vars = nn.Parameter(torch.zeros((task_num)))\n",
        "\n",
        "    def forward(self, preds, attrs):\n",
        "\n",
        "        bce = nn.BCELoss()\n",
        "        crossEntropy = nn.CrossEntropyLoss()\n",
        "\n",
        "        loss_age = crossEntropy(preds[0], attrs[:,0])\n",
        "\n",
        "        binary_losses = 0\n",
        "        for idx in range(1, len(preds)):\n",
        "            binary_losses += bce(preds[idx], attrs[:, idx].unsqueeze(1).to(torch.float32))\n",
        "        return loss_age + binary_losses\n",
        "\n",
        "class TripletLoss(nn.Module):\n",
        "    \"\"\"Triplet loss with hard positive/negative mining.\n",
        "    Reference:\n",
        "    Hermans et al. In Defense of the Triplet Loss for Person Re-Identification. arXiv:1703.07737.\n",
        "    Code imported from https://github.com/Cysu/open-reid/blob/master/reid/loss/triplet.py.\n",
        "    Args:\n",
        "        margin (float): margin for triplet.\n",
        "    \"\"\"\n",
        "    def __init__(self, margin=0.3, mutual_flag = False):\n",
        "        super(TripletLoss, self).__init__()\n",
        "        self.margin = margin\n",
        "        self.ranking_loss = nn.MarginRankingLoss(margin=margin)\n",
        "        self.mutual = mutual_flag\n",
        "\n",
        "    def forward(self, inputs, targets, centroids_batch):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs: feature matrix with shape (batch_size, feat_dim)\n",
        "            targets: ground truth labels with shape (num_classes)\n",
        "        \"\"\"\n",
        "        #print(centroids_batch)\n",
        "        #print(inputs.size())\n",
        "        str_targets = targets\n",
        "        targets = torch.Tensor(np.array([int(el) for el in targets]))\n",
        "        n = inputs.size(0)\n",
        "        # inputs = 1. * inputs / (torch.norm(inputs, 2, dim=-1, keepdim=True).expand_as(inputs) + 1e-12)\n",
        "        # Compute pairwise distance, replace by the official when merged\n",
        "\n",
        "        dist = torch.pow(inputs, 2).sum(dim=1, keepdim=True).expand(n, n)\n",
        "        dist = dist + dist.t()\n",
        "        dist.addmm_(1, -2, inputs, inputs.t())\n",
        "        dist = dist.clamp(min=1e-12).sqrt()  # for numerical stability\n",
        "        # For each anchor, find the hardest positive and negative\n",
        "        mask = targets.expand(n, n).eq(targets.expand(n, n).t())\n",
        "        dist_ap, dist_an = [], []\n",
        "        p_centroid = []\n",
        "        for i in range(n):\n",
        "            identity = str_targets[i]\n",
        "            p_centroid.append(distance.euclidean(inputs[i].detach().cpu().numpy(), centroids_batch[identity]))\n",
        "            dist_ap.append(dist[i][mask[i]].max().unsqueeze(0))\n",
        "            dist_an.append(dist[i][mask[i] == 0].min().unsqueeze(0))\n",
        "\n",
        "        dist_ap = torch.cat(dist_ap)\n",
        "        dist_an = torch.cat(dist_an)\n",
        "        # Compute ranking hinge loss\n",
        "        y = torch.ones_like(dist_an)\n",
        "\n",
        "        #loss = self.ranking_loss(dist_an, dist_ap, y)\n",
        "        loss = self.ranking_loss(dist_an, torch.from_numpy(np.array(p_centroid)).to(device), y)\n",
        "        if self.mutual:\n",
        "            return loss, dist\n",
        "        return loss"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqtuT4NEPu_a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577,
          "referenced_widgets": [
            "60f514a2a7e64222825cacb363d4027b",
            "a16dbe65e8354b658c5eb574ea6ef31a",
            "b089fb3a178a4e3884c96bc8b9f629f6",
            "a4fb5b9cf90d4ed3b7854ca84048bb21",
            "ef464d546ddc4d029a1239afe89754ed",
            "ed5554759d5f43a3b791712337ae215d",
            "d86962b652d3461ca9eb0f5fa3cc6839",
            "9b73eed462c74c7eb80916017f1dc876",
            "5fcf5a24e1b34e26a711f8354b01deaf",
            "aa0c8aa623b341bd9c4e38087682202c",
            "3508552a29e84ca98b906f41e4a9dc21",
            "0474d8e2057b4fea95721a97bf9667ee",
            "adca8a999bf84ab18f68e276aa8e20d6",
            "ee44ceae0bc6473699351a565795a2fd",
            "a53ebf9f2f3d429494eb0880436037ea",
            "67ae3765b4cc4b9197d2d1c39c32b6af"
          ]
        },
        "outputId": "33443819-38df-4d5a-e888-a099456fb561"
      },
      "source": [
        "model = model_pipeline(config)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.32<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">icy-tree-60</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/dl_unitn/dl_project\" target=\"_blank\">https://wandb.ai/dl_unitn/dl_project</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/dl_unitn/dl_project/runs/2oy6m3qh\" target=\"_blank\">https://wandb.ai/dl_unitn/dl_project/runs/2oy6m3qh</a><br/>\n",
              "                Run data is saved locally in <code>/content/deeplearning_unitn/wandb/run-20210625_151741-2oy6m3qh</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "{'wandb': True, 'device': 'auto', 'train_root': '/content/dataset/train', 'test_root': '/content/dataset/test', 'queries_root': '/content/dataset/queries', 'attributes_file': '/content/dataset/annotations_train.csv', 'dataset': 'Market1501', 'backbone': 'resnet50', 'split': {'full_training_size': 0.75, 'train_size': 0.8}, 'compose': {'resize_h': 224, 'resize_w': 224}, 'epochs': 10, 'training_batch_size': 16, 'validation_batch_size': 32, 'learning_rate': 0.01, 'weight_decay': 1e-06, 'momentum': 0.9, 'test_before_training': False, 'test_after_epochs': 10, 'mAP_rank': 15}\n",
            "Extract queries proportion: 0.11\n",
            "Identities in train set: 563\n",
            "Identities in validation set: 188\n",
            "Train set size: 9631\n",
            "Validation set size: 2988\n",
            "Number of validation queries: 370\n",
            "Number of attributes: 27\n",
            "Backbone feature size: 2048\n",
            "Using resnet50 as backbone\n",
            "Training...\n",
            "<torch.utils.data.dataloader.DataLoader object at 0x7fe827fdee50>\n",
            "Computing centroids\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "60f514a2a7e64222825cacb363d4027b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=602.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5fcf5a24e1b34e26a711f8354b01deaf",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:65: UserWarning: This overload of addmm_ is deprecated:\n",
            "\taddmm_(Number beta, Number alpha, Tensor mat1, Tensor mat2)\n",
            "Consider using one of the following signatures instead:\n",
            "\taddmm_(Tensor mat1, Tensor mat2, *, Number beta, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:1025.)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: Loss after 00784 examples: 26.820\n",
            "Epoch 0: Loss after 01584 examples: 20.848\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNEmFFbzjPuZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaR4uXihvAn3"
      },
      "source": [
        "a = torch.rand((32, 512))\n",
        "b = torch.rand((563, 512))\n",
        "\n",
        "dist = torch.cdist(a, b, p=2)\n",
        "\n",
        "negative = min (dist[0]) that is not the same id\n",
        "positive = max (dist[0]) that is the same id"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}